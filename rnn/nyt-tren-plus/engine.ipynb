{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade nltk\n",
    "#!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=6, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed, 14 Dec 2016 07:23:43\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2141667 tokens)\n",
      "Test set: 32513 sentences (1070297 tokens)\n",
      "dev set: 32513 sentences (1068442 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Wed, 14 Dec 2016 07:24:50\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.new.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0   76   94  306   16   18    6   35 5101  890   13 3719  285 3875   15\n",
      " 2337 8384 7771 9852    7 4191 6318 1219   12   54   28  567   21   53   49\n",
      "    6  548 1216   50   59   65   48    7   76  336   63  211  538   15  165\n",
      "   36  676   37    0    4 2243  112   11  124 1819  239    3   13  630    3\n",
      "   16  760   11 4791    3   33 8556 3697  234   10 1634 6716   12    2    4\n",
      " 4056    6 1262 1587 3040 4959    5    0    4 4274  370    2   17  318   11\n",
      "  351    6 2071   50 2010   61   43   41  474  658]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[   0   20  565    4 3153   19   14  150  248    9  836    3  101    4 4745\n",
      "   31 2886    5    0   20 2687    4  549   50 5409    5    0    4  325   18\n",
      "    4 1232   31   41 1153  990   11 1140   45    4  524   13 1280   75 1673\n",
      "   13    4  747  445  188   12   44  169    6   35    4  237 1124   62    7\n",
      "    4  430    5    0    4  442 3208  496   19    4  312 3583   12  880 1801\n",
      "    6    4 3154    5    0  343   63  412  103   43   64 3075   13  129    7\n",
      "  324  622   23   20    3    8   13 1720 5975    5]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[   0   20   47 3110    4 1221 1128  119   13   14  357   29 8121    3  280\n",
      "   82  357   25    3   54   18    3    8  191   41  777  394   11  331    5\n",
      "    0 4536  130   28   21  140   51    9  380   19 1600    3 2192  218   11\n",
      "  481  713  453    6   21  140   51    5    0  280  216  273    3 1429  319\n",
      "  183   56   33   85 1451 2398   16  139    7    4 1429  147   81  236   56\n",
      "  553   14  783 2282   78   52   13    4 2542  178 1848 2289    9  604  149\n",
      "    3  178 2842    5    0  990  821    2    3  117]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ [u'Microsoft', u'offered', u'the', u'fix', u'on', u'its', u'Web', u'site', u'in', u'December', u',', u'when', u'the', u'vulnerability', u'was', u'discovered', u'.']\n",
      " [u'Microsoft', u'views', u'the', u'ruling', u'more', u'narrowly', u'.']\n",
      " [u'The', u'officials', u'said', u'the', u'concern', u'was', u'not', u'Lockheed', u'Martin', u\"'s\", u'size', u'but', u'the', u'potential', u'for', u'creating', u'one', u'supplier', u'for', u'the', u'advanced', u'electronic', u'systems', u'that', u'are', u'expected', u'to', u'be', u'the', u'big', u'military', u'business', u'of', u'the', u'future', u'.']\n",
      " [u'The', u'five', u'KPMG', u'partners', u'on', u'the', u'Xerox', u'audit', u'that', u'remain', u'subject', u'to', u'the', u'S.E.C', u'.']\n",
      " [u\"''The\", u'last', u'four', u'years', u'have', u'been', u'challenging', u'for', u'all', u'of', u'us', u'here', u'at', u'Microsoft', u',', u'and', u'for', u'me', u'personally', u'.']\n",
      " [u'Responding', u'to', u'defeat', u'with', u'the', u'pride', u'and', u'tenacity', u'of', u'a', u'champion', u',', u'the', u'I.B.M', u'.']\n",
      " [u'On', u'Tuesday', u'Mr.', u'Jobs', u'drew', u'a', u'parallel', u'between', u'the', u'introduction', u'of', u'movies', u'and', u'Apple', u\"'s\", u'experience', u'with', u'television', u',', u'where', u'it', u'also', u'started', u'with', u'a', u'single', u'studio', u'and', u'has', u'now', u'been', u'widely', u'embraced', u'.']\n",
      " [u'In', u'the', u'current', u'market', u',', u'the', u'News', u'Corporation', u'is', u'the', u'only', u'network', u'owner', u'with', u'major', u'sports', u'properties', u'that', u'has', u'written', u'down', u'investments', u'in', u'sports', u'like', u'football', u'.']\n",
      " [u'The', u'quarter', u'reflected', u'mixed', u'results', u'for', u'Oracle', u\"'s\", u'two', u'main', u'software', u'businesses', u',', u'database', u'and', u'applications', u'.']\n",
      " [u'The', u'ad', u'then', u'noted', u'that', u'Verizon', u'wants', u'34,600', u'retirees', u',', u'who', u'now', u'contribute', u'little', u'toward', u'health', u'coverage', u',', u'to', u'pay', u'$', u'60', u'million', u'more', u'for', u'health', u'benefits', u'each', u'year', u'through', u'higher', u'premiums', u'and', u'co-payments', u'.']\n",
      " [u'The', u'big', u'market', u'for', u'computer', u'server', u'software', u'is', u'also', u'crucial', u'to', u'Microsoft', u\"'s\", u'future', u'.']\n",
      " [u'Aetna', u'said', u'it', u'was', u'also', u'cutting', u'1,120', u'jobs', u'from', u'regional', u'operations', u',', u'including', u'sales', u',', u'and', u'580', u'jobs', u'in', u'other', u'areas', u',', u'including', u'information', u'technology', u'and', u'human', u'resources', u'.']\n",
      " [u'While', u'the', u'Red', u'Hat', u'offering', u'coincided', u'with', u'a', u'better', u'than', u'3', u'percent', u'rise', u'in', u'the', u'Nasdaq', u'today', u',', u'it', u'also', u'came', u'on', u'the', u'heels', u'of', u'a', u'brutal', u'correction', u'in', u'Internet', u'stocks', u'in', u'the', u'last', u'two', u'weeks', u'.']\n",
      " [u'While', u'A.T.', u'&', u'T.', u'and', u'McCaw', u'could', u'simply', u'choose', u'to', u'sell', u'or', u'trade', u'the', u'cellular', u'properties', u'that', u'prompted', u'today', u\"'s\", u'ruling', u',', u'executives', u'at', u'both', u'companies', u'said', u'today', u'that', u'they', u'would', u'fight', u'to', u'keep', u'them', u'.']\n",
      " [u'At', u'the', u'same', u'time', u',', u'CNN', u'is', u'expected', u'to', u'come', u'under', u'the', u'ownership', u'of', u'a', u'new', u'company', u'this', u'fall', u',', u'when', u'its', u'parent', u'corporation', u',', u'Time', u'Warner', u',', u'is', u'scheduled', u'to', u'merge', u'officially', u'with', u'America', u'Online', u'.']\n",
      " [u'Last', u'month', u'analysts', u'were', u'speculating', u'that', u'Campbell', u'was', u'considering', u'selling', u'its', u'Swanson', u'frozen', u'food', u'business', u',', u'reducing', u'its', u'Camden', u',', u'N.J.', u',', u'headquarters', u'staff', u'and', u'closing', u'one', u'of', u'its', u'four', u'soup', u'plants', u'in', u'Napoleon', u',', u'Ohio', u';', u'Paris', u',', u'Tex', u'.']\n",
      " [u'Oracle', u'Corp', u'co-president', u'Safra', u'Catz', u'testifies', u'she', u'expects', u'PeopleSoft', u\"'s\", u'earnings', u'will', u'continue', u'to', u'drop', u',', u'and', u'while', u'she', u'suggests', u'that', u'Oracle', u'might', u'lower', u'its', u'hostile', u'bid', u'of', u'$', u'21', u'per', u'share', u',', u'she', u'acknowledges', u'that', u'new', u'price', u'is', u'not', u'before', u'board', u',', u'trial', u',', u'where', u'Oracle', u'has', u'challenged', u'PeopleSoft', u\"'s\", u'anti-takeover', u'defenses', u';', u'at', u'issue', u'are', u'competing', u'claims', u'of', u'software', u'rivals', u'(', u'M', u')']\n",
      " [u\"''But\", u'I.B.M', u'.']\n",
      " [u'Lycos', u',', u'the', u'10-year-old', u'search', u'company', u'based', u'in', u'Waltham', u',', u'Mass.', u',', u'searches', u'at', u'least', u'20', u'bulletin', u'boards', u'and', u'missing', u'persons', u'Web', u'sites', u'every', u'four', u'hours', u'to', u'capture', u'data', u'for', u'its', u'service', u'(', u'lycos.com/katrina', u')', u',', u'while', u'Yahoo', u'retrieves', u'information', u'every', u'hour', u'from', u'15', u'large', u'sites', u'and', u'many', u'smaller', u'ones', u'for', u'its', u'engine', u'(', u'news.yahoo.com/katrinahelp', u')', u'.']\n",
      " [u'Like', u'many', u'of', u'Apple', u\"'s\", u'strategic', u'moves', u',', u'the', u'implication', u'of', u'an', u'Apple', u'browser', u'for', u'Windows', u'was', u'not', u'immediately', u'clear', u'.']\n",
      " [u'Still', u',', u'in', u'some', u'towns', u'in', u'Texas', u'where', u'Verizon', u'started', u'selling', u'television', u'service', u'last', u'year', u',', u'the', u'reception', u'has', u'been', u'positive', u';', u'about', u'one-third', u'of', u'the', u'homes', u'offered', u'it', u'signed', u'up', u'.']\n",
      " [u\"''The\", u'danger', u'to', u'Boeing', u'is', u'that', u'its', u'enemies', u'in', u'Washington', u'will', u'use', u'this', u'and', u'take', u'advantage', u'of', u'it', u',', u\"''\", u'he', u'said', u'.']\n",
      " [u'Fox', u'News', u'Channel', u'is', u'trying', u'to', u'draw', u'audience', u'with', u'weekly', u'concerts', u'outside', u'News', u'Corp', u\"'s\", u'headquarters', u'in', u'midtown', u'Manhattan', u'(', u'S', u')']\n",
      " [u'Intended', u'to', u'broaden', u'Mattel', u\"'s\", u'reach', u',', u'the', u'unit', u'has', u'instead', u'contributed', u'to', u'$', u'300', u'million', u'in', u'losses', u',', u'a', u'$', u'7', u'billion', u'drop', u'in', u'the', u'company', u\"'s\", u'market', u'valuation', u'and', u'the', u'resignation', u'of', u'Jill', u'E.', u'Barad', u'as', u'chief', u'executive', u'.']\n",
      " [u'In', u'an', u'interview', u',', u'H.', u'Lee', u'Scott', u'Jr.', u',', u'Wal-Mart', u\"'s\", u'chief', u'executive', u',', u'said', u'that', u\"''it\", u'has', u'reached', u'the', u'point', u'where', u'there', u'are', u'issues', u'that', u'take', u'specialized', u'skills', u'to', u'get', u'to', u'the', u'bottom', u'of', u'.', u\"''\"]\n",
      " [u'While', u'Hewlett', u'is', u'a', u'partner', u'in', u'the', u'development', u',', u'most', u'computer', u'executives', u'agree', u'that', u'Intel', u'is', u'driving', u'the', u'project', u'.', u')']\n",
      " [u\"''The\", u'market', u'has', u'simply', u'moved', u'beyond', u'Amex', u',', u\"''\", u'said', u'Allen', u'R.', u'DeCotiis', u',', u'president', u'of', u'Payment', u'System', u'Inc.', u',', u'a', u'leading', u'credit', u'card', u'research', u'firm', u'once', u'owned', u'by', u'American', u'Express', u'.']\n",
      " [u'Microsoft', u'has', u'not', u'said', u'how', u'much', u'those', u'products', u'would', u'cost', u'.']\n",
      " [u'Technology', u'Briefing', u'|', u'Hardware', u':', u'Kodak', u'To', u'Buy', u'Scitex', u'Printing', u'Unit']\n",
      " [u'Lawyers', u'for', u'the', u'Intel', u'Corporation', u'should', u'be', u'well', u'acquainted', u'with', u'antitrust', u'inquiries', u'by', u'the', u'Federal', u'Trade', u'Commission', u'--', u'a', u'process', u'different', u'from', u'the', u'one', u'followed', u'in', u'Justice', u'Department', u'antitrust', u'cases', u'--', u'because', u'the', u'commission', u'has', u'been', u'investigating', u'the', u'microprocessor', u'giant', u'off', u'and', u'on', u'throughout', u'this', u'decade', u'.']]\n",
      "[['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['0']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']]\n",
      "2206693 2206693 1102811 1102811 1100956 1100956\n"
     ]
    }
   ],
   "source": [
    "print train_ids[:100]\n",
    "print train_sids[:100]\n",
    "print test_ids[:100]\n",
    "print test_sids[:100]\n",
    "print dev_ids[:100]\n",
    "print dev_sids[:100]\n",
    "print test_sents[:30]\n",
    "print test_sentis[:30]\n",
    "print len(train_ids), len(train_sids), len(test_ids), len(test_sids), len(dev_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = 'tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed, 14 Dec 2016 07:26:53\n",
      "WARNING:tensorflow:From <ipython-input-9-66f68d545e13>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-66f68d545e13>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:31:29\n",
      "[epoch 1] Completed in 0:04:34\n",
      "[epoch 1] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.726  (perplexity: 2.07)\n",
      "[epoch 1] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.722  (perplexity: 2.06)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:37:17\n",
      "[epoch 2] Completed in 0:04:32\n",
      "[epoch 2] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.718  (perplexity: 2.05)\n",
      "[epoch 2] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.717  (perplexity: 2.05)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:43:07\n",
      "[epoch 3] Completed in 0:04:36\n",
      "[epoch 3] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "[epoch 3] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.715  (perplexity: 2.04)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:48:55\n",
      "[epoch 4] Completed in 0:04:30\n",
      "[epoch 4] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.708  (perplexity: 2.03)\n",
      "[epoch 4] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:54:43\n",
      "[epoch 5] Completed in 0:04:34\n",
      "[epoch 5] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.703  (perplexity: 2.02)\n",
      "[epoch 5] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "\n",
      "Wed, 14 Dec 2016 07:55:58\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, 'tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#skip unless mixed\n",
    "# Same as above, but as a batch\n",
    "import collections\n",
    "reload(rnnsm)\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, './' + trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  #print w\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "  #print w\n",
    "  svocab=vocabulary.Vocabulary(True)\n",
    "  for row in w:\n",
    "    k = [sword_id for i, sword_id in enumerate(row)]\n",
    "    k = [svocab.id_to_word[sword_id] for sword_id in k]\n",
    "    #print collections.Counter(k)\n",
    "    #print svocab.id_to_word[sword_id],\n",
    "  #collections.Counter([svocab.id_to_word(sword_id) for i, sword_id in k])\n",
    "        \n",
    "  # Print generated sentences\n",
    "  #for row in w:\n",
    "  #  for i, word_id in enumerate(row):\n",
    "  #    print vocab.id_to_word[word_id],\n",
    "  #    if (i != 0) and (word_id == vocab.START_ID):\n",
    "  #      break\n",
    "  #  print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [utils.flatten(y)[0]]\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    seq = \n",
    "    print seq_predict(sm, session, ['must', 'sell'], vocab, svocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq_lm(sm, session, seq, vocab):\n",
    "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    \n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  feed_dict = {sm.input_w_:w,\n",
    "               sm.target_y_:y,\n",
    "               sm.initial_h_:h,\n",
    "               sm.dropout_keep_prob_: 1.0}\n",
    "  # Return log(P(seq)) = -1*loss\n",
    "  return -1*session.run(sm.loss_, feed_dict)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "      sm = rnnsm.RNNSM(**model_params)\n",
    "      sm.BuildCoreGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "  \n",
    "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "      inputs = [inputs]\n",
    "\n",
    "    # Actually run scoring\n",
    "    results = []\n",
    "    for words in inputs:\n",
    "      score = score_seq(sm, session, words, vocab)\n",
    "      results.append((score, words))\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort: results = sorted(results, reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    for score, words in results:\n",
    "      print \"\\\"%s\\\" : %.05f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 2665 which is outside the valid range of [0, 63).  Label values: 633 2665 10 67\n\t [[Node: model/loss/loss = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](model/loss/Reshape, model/loss/Reshape_1)]]\n\nCaused by op u'model/loss/loss', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-83-369449b7415d>\", line 3, in <module>\n    load_and_score([s.split() for s in sents])\n  File \"<ipython-input-82-b36be2bb82a5>\", line 20, in load_and_score\n    lm.BuildCoreGraph()\n  File \"rnnsm.py\", line 112, in BuildCoreGraph\n    self.loss_ = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits_, self.target_y_, name = \"loss\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1551, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2378, in _sparse_softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Received a label value of 2665 which is outside the valid range of [0, 63).  Label values: 633 2665 10 67\n\t [[Node: model/loss/loss = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](model/loss/Reshape, model/loss/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-369449b7415d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m sents = [\"once upon a time\",\n\u001b[0;32m      2\u001b[0m          \"the quick brown fox jumps over the lazy dog\"]\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mload_and_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-b36be2bb82a5>\u001b[0m in \u001b[0;36mload_and_score\u001b[1;34m(inputs, sort)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m       \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m       \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-b36be2bb82a5>\u001b[0m in \u001b[0;36mscore_seq\u001b[1;34m(sm, session, seq, vocab)\u001b[0m\n\u001b[0;32m     11\u001b[0m                sm.dropout_keep_prob_: 1.0}\n\u001b[0;32m     12\u001b[0m   \u001b[1;31m# Return log(P(seq)) = -1*loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_and_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 964\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1014\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Received a label value of 2665 which is outside the valid range of [0, 63).  Label values: 633 2665 10 67\n\t [[Node: model/loss/loss = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](model/loss/Reshape, model/loss/Reshape_1)]]\n\nCaused by op u'model/loss/loss', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-83-369449b7415d>\", line 3, in <module>\n    load_and_score([s.split() for s in sents])\n  File \"<ipython-input-82-b36be2bb82a5>\", line 20, in load_and_score\n    lm.BuildCoreGraph()\n  File \"rnnsm.py\", line 112, in BuildCoreGraph\n    self.loss_ = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits_, self.target_y_, name = \"loss\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1551, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2378, in _sparse_softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Received a label value of 2665 which is outside the valid range of [0, 63).  Label values: 633 2665 10 67\n\t [[Node: model/loss/loss = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](model/loss/Reshape, model/loss/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the boy and the girl are\" : -36.40670\n",
      "\"the boy and the girl is\" : -34.56690\n",
      "\"the boys are\" : -16.81372\n",
      "\"the boys is\" : -16.17620\n",
      "\"there are many deer\" : -26.15632\n",
      "\"there are many deers\" : -19.46089\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\",\n",
    "         \"the boy and the girl is\",\n",
    "         \n",
    "         \"the boys are\", \n",
    "         \"the boys is\",\n",
    "         \n",
    "         \"there are many deer\", \n",
    "         \"there are many deers\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In a good market, Apple's price falls\" : -39.03466\n",
      "\"In a good market, Apple's price rises\" : -41.51408\n",
      "\"In a good market, Apple's price stays\" : -37.77964\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\n",
    "         \"In a good market, Apple's price falls\",\n",
    "         \"In a good market, Apple's price rises\",\n",
    "    \"In a good market, Apple's price stays\"\n",
    "         ]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I have lots of green plastic square toys\" : -61.22992\n",
      "\"I have lots of green square plastic toys\" : -62.09601\n",
      "\"I have lots of plastic green square toys\" : -62.21913\n",
      "\"I have lots of square green plastic toys\" : -62.24586\n",
      "\"I have lots of plastic square green toys\" : -62.28750\n",
      "\"I have lots of square plastic green toys\" : -66.11003\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I have lots of green plastic flat toys\" : -57.62775\n",
      "\"I have lots of green flat plastic toys\" : -59.28275\n",
      "\"I have lots of plastic green flat toys\" : -59.49638\n",
      "\"I have lots of plastic flat green toys\" : -60.39898\n",
      "\"I have lots of flat plastic green toys\" : -60.96680\n",
      "\"I have lots of flat green plastic toys\" : -61.51214\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of Apple\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"stocks\", \"pies\", \"computers\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I have lots of green plastic round toys\" : -59.50684\n",
      "\"I have lots of plastic green round toys\" : -61.64307\n",
      "\"I have lots of green round plastic toys\" : -61.85661\n",
      "\"I have lots of plastic round green toys\" : -62.10223\n",
      "\"I have lots of round green plastic toys\" : -62.98605\n",
      "\"I have lots of round plastic green toys\" : -63.16181\n"
     ]
    }
   ],
   "source": [
    "prefix = \"The future of Apple \".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"round\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I have lots of small plastic green toys\" : -55.65118\n",
      "\"I have lots of small green plastic toys\" : -56.18058\n",
      "\"I have lots of green plastic small toys\" : -58.51525\n",
      "\"I have lots of plastic small green toys\" : -58.71899\n",
      "\"I have lots of green small plastic toys\" : -59.45413\n",
      "\"I have lots of plastic green small toys\" : -60.29629\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"small\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
