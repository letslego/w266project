{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade nltk\n",
    "#!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c8e7c32631d4>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c8e7c32631d4>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=6, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 20:43:20\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2143415 tokens)\n",
      "Test set: 32513 sentences (1069358 tokens)\n",
      "dev set: 32513 sentences (1067633 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sat, 17 Dec 2016 20:44:02\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.new.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  162   11  194  591   15  477 3589  862  676    2 4451 3145   13  209\n",
      "  483 2327 5990   15 5373    2   12 7726  132    2 2475 1492   26    2   15\n",
      " 1589   15  165   36   55   37    0    9  685    3  215   18   22   42  647\n",
      " 1811    8  941 6808    6  805    6  482   14  566  392   26   21   65   51\n",
      "   26   40    3  376   22 2042    6  143  245  137  219   22   42 1212  511\n",
      "    5    0  312 1016    2   27  225   27   10   34  247 2047   12  161   22\n",
      "  316 1819    6  213 4028 3666    5    0  157    4]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0  898 2085   16  526 6305    8 2264   13  324    3   30   18 6063 1370\n",
      " 5575    3 5062    8   69   71    7  913    3   33   16  222    9 3757    2\n",
      "    3 2603    5    0  209    7  324   61   47 1008    9    4 1065  932  516\n",
      "   12    4  831   31   41 3957   39 2380   95    4 5376  116  399   36    8\n",
      " 1351   37    9  450   13 6386  190  118  892    5    0    4  728  229   92\n",
      "    7    4  329 2968 3246   60    9  604  149  886   77   27    4 4633 3246\n",
      "   91  928   14  168   13    4 2133 9934 1247 2968]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0  223   46   25   28 1560   39  861   17   10  479 2243   25    6  725\n",
      "  636  363   10 1183 3359   34  119 1045   23 4840 7694  233 8251  704    9\n",
      " 2983    5    0  263  158  957 1150   45 1916  738    0  263  974   21  172\n",
      "   49   13    4  106 1186  251    7   40    3  266    6    2    3   10  181\n",
      "    7 4450 3901 9347   12 2818  858  719    5    0 2250    3  555   80    5\n",
      "    0   22   31    4 4308   12   38  869   16   75    7    4  393 1768  804\n",
      "    2    9   94  629   12  559   10  352 1213    9]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[ [u\"''This\", u'situation', u'is', u'very', u'painful', u'and', u'disappointing', u'for', u'us', u',', u\"''\", u'said', u'Jill', u'E.', u'Barad', u',', u'chairwoman', u'and', u'chief', u'executive', u'of', u'Mattel', u',', u'which', u'is', u'based', u'in', u'El', u'Segundo', u',', u'Calif', u'.']\n",
      " [u'Those', u'of', u'us', u'who', u'had', u'clients', u'in', u'the', u'Prudential', u'cases', u'found', u'that', u'the', u'process', u'was', u'not', u'necessarily', u'an', u'improvement', u'over', u'the', u'arbitration', u'system', u'then', u'(', u'and', u'currently', u')', u'in', u'place', u'for', u'resolving', u'investors', u\"'\", u'claims', u'.']\n",
      " [u'The', u'battle', u'between', u'two', u'of', u'the', u'top', u'gold', u'mining', u'companies', u'in', u'North', u'America', u'continued', u'yesterday', u'as', u'the', u'Newmont', u'Mining', u'Corporation', u'raised', u'its', u'offer', u'for', u'the', u'Santa', u'Fe', u'Pacific', u'Gold', u'Corporation', u'to', u'$', u'2.15', u'billion', u'in', u'an', u'attempt', u'to', u'break', u'up', u'the', u'merger', u'agreement', u'between', u'Santa', u'Fe', u'and', u'the', u'Homestake', u'Mining', u'Company', u'.']\n",
      " [u'Pfizer', u'executives', u'have', u'said', u'the', u'company', u'intends', u'to', u'look', u'at', u'deals', u'of', u'all', u'sizes', u',', u'from', u'individual', u'licensing', u'agreements', u'to', u'large-scale', u'takeovers', u'.']\n",
      " [u'Landmark', u'settlement', u'in', u'January', u'that', u'had', u'10', u'former', u'WorldCom', u'directors', u'agreeing', u'to', u'pay', u'$', u'18', u'million', u'from', u'their', u'own', u'pockets', u'to', u'investors', u'who', u'lost', u'money', u'in', u'company', u\"'s\", u'failure', u'is', u'scuttled', u';', u'settlement', u'falls', u'apart', u'after', u'Federal', u'Judge', u'Denise', u'Cote', u'rules', u'that', u'one', u'aspect', u'of', u'deal', u'is', u'illegal', u'because', u'it', u'would', u'have', u'limited', u'directors', u\"'\", u'potential', u'liability', u'and', u'exposed', u'investment', u'banks', u'that', u'are', u'also', u'defendants', u'in', u'case', u'to', u'greater', u'damages', u';', u'settlement', u'would', u'also', u'have', u'prevented', u'banks', u',', u'including', u'J', u'P', u'Morgan', u'Chase', u',', u'Deutsche', u'Bank', u'and', u'Bank', u'of', u'America', u',', u'from', u'being', u'able', u'to', u'sue', u'directors', u'and', u'possibly', u'recover', u'money', u'from', u'them', u';', u'Alan', u'G', u'Hevesi', u',', u'trustee', u'of', u'New', u'York', u'State', u'Common', u'Retirement', u'Fund', u'and', u'lead', u'plaintiff', u'in', u'lawsuit', u',', u'says', u'he', u'respects', u'judge', u\"'s\", u'ruling', u'but', u'provision', u'is', u'necessary', u';', u'Hevesi', u'is', u'seeking', u'$', u'13', u'billion', u'in', u'damages', u'for', u'investors', u'who', u'bought', u'WorldCom', u'securities', u'in', u'two', u'years', u'before', u'it', u'filed', u'for', u'bankruptcy', u'protection', u'in', u'2002', u';', u'case', u'is', u'scheduled', u'to', u'go', u'to', u'trial', u'Feb', u'28', u'(', u'M', u')']\n",
      " [u'He', u'described', u'it', u'as', u\"''the\", u'best', u'99', u'cents', u'I', u\"'ve\", u'ever', u'spent', u\"''\", u'in', u'an', u'e-mail', u'message', u'last', u'week', u'to', u'Jack', u'Miller', u',', u'the', u'editor', u'in', u'chief', u'of', u'the', u'news', u'and', u'gossip', u'Web', u'site', u'As', u'the', u'Apple', u'Turns', u'(', u'www.appleturns.com', u')', u'.']\n",
      " [u'For', u'the', u'quarter', u'ended', u'Sept.', u'29', u',', u'Apple', u'earned', u'18', u'cents', u'a', u'share', u',', u'excluding', u'a', u'one-time', u'tax', u'gain', u'.']\n",
      " [u'Annual', u'analysis', u'by', u'Jupiter', u'Media', u'Metrix', u'of', u'Web', u'site', u'traffic', u'surrounding', u'Super', u'Bowl', u'shows', u'big', u'increases', u'for', u'site', u'of', u'advertisers', u'AT', u'&', u'T', u'Wireless', u'and', u'site', u'set', u'up', u'by', u'Yahoo', u'and', u'Pepsi-Cola', u'division', u'of', u'PepsiCo', u'(', u'S', u')']\n",
      " [u'What', u'concerns', u'the', u'state', u'attorneys', u'general', u'is', u'that', u'in', u'its', u'new', u'strategies', u',', u'known', u'as', u'.Net', u'and', u'Hailstorm', u',', u'Microsoft', u'has', u'been', u'aggressively', u'pushing', u'to', u'link', u'its', u'new', u'consumer', u'and', u'business', u'offerings', u'to', u'the', u'next', u'version', u'of', u'its', u'operating', u'system', u',', u'Windows', u'XP', u'.']\n",
      " [u'AT', u'&', u'T', u'endorsed', u'yesterday', u\"'s\", u'proposal', u',', u'as', u'did', u'MCI', u'Communications', u'and', u'the', u'Sprint', u'Corporation', u'.']\n",
      " [u\"''It\", u\"'s\", u'not', u'that', u'we', u'think', u'they', u'do', u\"n't\", u'have', u'the', u'right', u'to', u'call', u'themselves', u'the', u'United', u'States', u'Polo', u'Association', u',', u'because', u'of', u'course', u'they', u'do', u',', u\"''\", u'said', u'Anthony', u'Lo', u'Cicero', u',', u'a', u'lawyer', u'for', u'Polo', u'Ralph', u'Lauren', u'in', u'New', u'York', u'.']\n",
      " [u'Molson', u'USA', u'in', u'Golden', u',', u'Colo.', u',', u'part', u'of', u'the', u'Canadian', u'brewer', u'Molson', u'Inc.', u',', u'selected', u'Crispin', u'Porter', u'&', u'Bogusky', u'in', u'Miami', u',', u'which', u'is', u'49', u'percent', u'owned', u'by', u'the', u'Maxxcom', u'unit', u'of', u'the', u'MDC', u'Corporation', u',', u'to', u'handle', u'its', u'account', u'.']\n",
      " [u'A', u'spokesman', u'for', u'Interpublic', u',', u'whose', u'agencies', u'handle', u'advertising', u'for', u'Coca-Cola', u',', u'General', u'Motors', u'and', u'Nestl\\xe9', u',', u'declined', u'to', u'comment', u'beyond', u'the', u'statement', u'.']\n",
      " [u'Glenn', u'Bozarth', u',', u'a', u'spokesman', u'for', u'Mattel', u',', u'said', u'the', u'company', u'wished', u'Mr.', u'Haddad', u'well', u'.']\n",
      " [u'A', u'collective', u'shudder', u'passed', u'through', u'Wall', u'Street', u'this', u'week', u'when', u'Mannesmann', u'A.G.', u'sought', u'to', u'remove', u'Goldman', u'Sachs', u'Group', u'as', u'the', u'corporate', u'adviser', u'to', u'Vodafone', u'Airtouch', u'P.L.C.', u',', u'the', u'British', u'telecommunications', u'company', u'that', u'has', u'made', u'a', u'hostile', u'bid', u'for', u'the', u'German', u'concern', u'.']\n",
      " [u'New', u'Motorola', u'Task', u'Goes', u'to', u'Leo', u'Burnett']\n",
      " [u'Amgen', u',', u'for', u'instance', u',', u'gave', u'different', u'potential', u'arthritis', u'drugs', u'to', u'human', u'volunteers', u'and', u'then', u'used', u'a', u'blood', u'test', u'to', u'gauge', u'the', u'best', u'one', u'to', u'take', u'forward', u'into', u'clinical', u'trials', u'.']\n",
      " [u'Procter', u'&', u'Gamble', u'says', u'it', u'will', u'cut', u'15,000', u'jobs', u'worldwide', u',', u'13', u'percent', u'of', u'work', u'force', u',', u'close', u'10', u'plants', u'and', u'take', u'nearly', u'$', u'2', u'billion', u'in', u'charges', u'linked', u'to', u'reorganization', u'in', u'effort', u'to', u'loosen', u'its', u'stiff', u'corporate', u'culture', u'and', u'free', u'more', u'cash', u'for', u'product', u'development', u';', u'says', u'financial', u'results', u'for', u'current', u'quarter', u'will', u'be', u'flat', u'and', u'will', u'improve', u'only', u'modestly', u'next', u'quarter', u';', u'admits', u'it', u'was', u'too', u'ambitious', u'in', u'10-year', u'goal', u'to', u'double', u'revenue', u'to', u'$', u'70', u'billion', u'by', u'2005', u';', u'chief', u'executive', u'Durk', u'I', u'Jager', u'says', u'reorganization', u'will', u'spur', u'future', u'growth', u'and', u'save', u'some', u'$', u'900', u'million', u'a', u'year', u'by', u'2004', u'(', u'M', u')']\n",
      " [u'made', u'the', u'announcement', u'yesterday', u'in', u'a', u'letter', u'to', u'the', u'Center', u'for', u'Democracy', u'and', u'Technology', u',', u'a', u'Washington-based', u'privacy', u'group', u'that', u'has', u'asked', u'the', u'nation', u\"'s\", u'major', u'computer', u'makers', u'to', u'provide', u'computer', u'users', u'more', u'security', u'than', u'is', u'offered', u'by', u'Intel', u\"'s\", u'software', u'remedy', u'.']\n",
      " [u'One', u'reason', u'for', u'the', u'tepid', u'reaction', u'was', u'that', u'some', u'analysts', u'wonder', u'whether', u'Kodak', u',', u'which', u'lost', u'more', u'than', u'$', u'100', u'million', u'in', u'digital', u'products', u'in', u'the', u'first', u'half', u'of', u'1997', u',', u'is', u'facing', u'even', u'worse', u'numbers', u'in', u'the', u'second', u'half', u'.']\n",
      " [u\"''As\", u'he', u'came', u'into', u'power', u'Intel', u'tried', u'to', u'become', u'a', u'more', u'aggressive', u'marketing', u'company', u',', u\"''\", u'he', u'said', u'.']\n",
      " [u'Living', u'and', u'Learning', u'At', u'Dishwasher', u'U.', u';', u'Whirlpool', u'Trainees', u'Prepare', u'for', u'Real', u'World']\n",
      " [u'While', u'Boeing', u'would', u'not', u'discuss', u'the', u'reason', u'for', u'Mr.', u'Womack', u\"'s\", u'departure', u',', u'it', u'said', u'it', u'remained', u'committed', u'to', u'lean', u'manufacturing', u'.']\n",
      " [u'During', u'the', u'quarter', u',', u'Cisco', u',', u'which', u'is', u'based', u'in', u'San', u'Jose', u',', u'Calif.', u',', u'completed', u'the', u'acquisitions', u'of', u'Aironet', u'Wireless', u'Communications', u'and', u'Pirelli', u'Optical', u'Systems', u'for', u'a', u'combined', u'total', u',', u'including', u'assumed', u'liabilities', u',', u'of', u'about', u'$', u'2.85', u'billion', u'.']\n",
      " [u'In', u'a', u'telephone', u'conference', u'call', u'with', u'market', u'analysts', u'yesterday', u',', u'A.T.', u'&', u'T.', u'executives', u'said', u'that', u'almost', u'all', u'current', u'cellular', u'calls', u'are', u'placed', u'through', u'local', u'systems', u'.']\n",
      " [u'The', u'deal', u',', u'which', u'will', u'give', u'Con', u'Edison', u'access', u'to', u'lower-cost', u'electricity', u',', u'is', u'the', u'company', u\"'s\", u'biggest', u'move', u'so', u'far', u'to', u'protect', u'its', u'base', u'of', u'three', u'million', u'customers', u',', u'who', u'will', u'be', u'able', u'to', u'buy', u'power', u'at', u'lower', u'rates', u'from', u'outside', u'companies', u'as', u'New', u'York', u'deregulates', u'its', u'utility', u'market', u'.']\n",
      " [u'The', u'announcement', u'did', u'not', u'give', u'a', u'date', u'for', u'the', u'debut', u'of', u'the', u'new', u'Yahoo', u'Mail', u'release', u'.']\n",
      " [u'In', u'Nasdaq', u'trading', u'today', u',', u'the', u'shares', u'of', u'Apple', u'rose', u'87.5', u'cents', u',', u'to', u'close', u'at', u'$', u'18.75', u'.']\n",
      " [u'A', u'spokeswoman', u'for', u'Microsoft', u'said', u'the', u'company', u'had', u'blocked', u\"''many\", u'sites', u\"''\", u'in', u'China', u'.']\n",
      " [u'Advances', u'in', u'computer', u'design', u'and', u'manufacturing', u'also', u'mean', u'that', u'Boeing', u'can', u'save', u'money', u'developing', u'the', u'plane', u'and', u'on', u'each', u'copy', u'that', u'it', u'makes', u'.']]\n",
      "[['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']]\n",
      "2208441 2208441 1101872 1101872 1100147 1100147\n"
     ]
    }
   ],
   "source": [
    "print train_ids[:100]\n",
    "print train_sids[:100]\n",
    "print test_ids[:100]\n",
    "print test_sids[:100]\n",
    "print dev_ids[:100]\n",
    "print dev_sids[:100]\n",
    "print test_sents[:30]\n",
    "print test_sentis[:30]\n",
    "print len(train_ids), len(train_sids), len(test_ids), len(test_sids), len(dev_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = './tf_saved/tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 20:44:29\n",
      "WARNING:tensorflow:From <ipython-input-10-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 20:49:01\n",
      "[epoch 1] Completed in 0:04:31\n",
      "[epoch 1] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.719  (perplexity: 2.05)\n",
      "[epoch 1] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.718  (perplexity: 2.05)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 20:54:48\n",
      "[epoch 2] Completed in 0:04:35\n",
      "[epoch 2] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.712  (perplexity: 2.04)\n",
      "[epoch 2] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.715  (perplexity: 2.04)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:00:33\n",
      "[epoch 3] Completed in 0:04:32\n",
      "[epoch 3] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.706  (perplexity: 2.03)\n",
      "[epoch 3] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.714  (perplexity: 2.04)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:06:11\n",
      "[epoch 4] Completed in 0:04:30\n",
      "[epoch 4] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.701  (perplexity: 2.02)\n",
      "[epoch 4] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.714  (perplexity: 2.04)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:11:52\n",
      "[epoch 5] Completed in 0:04:30\n",
      "[epoch 5] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.694  (perplexity: 2.00)\n",
      "[epoch 5] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:17:35\n",
      "[epoch 6] Completed in 0:04:31\n",
      "[epoch 6] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.686  (perplexity: 1.99)\n",
      "[epoch 6] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.715  (perplexity: 2.04)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:23:24\n",
      "[epoch 7] Completed in 0:04:41\n",
      "[epoch 7] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.676  (perplexity: 1.97)\n",
      "[epoch 7] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:29:23\n",
      "[epoch 8] Completed in 0:04:45\n",
      "[epoch 8] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.664  (perplexity: 1.94)\n",
      "[epoch 8] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:35:13\n",
      "[epoch 9] Completed in 0:04:38\n",
      "[epoch 9] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.651  (perplexity: 1.92)\n",
      "[epoch 9] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.724  (perplexity: 2.06)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "in batch_generator 2208441 2208441 2208400 2208400 2208400 50\n",
      "Sat, 17 Dec 2016 21:40:57\n",
      "[epoch 10] Completed in 0:04:34\n",
      "[epoch 10] in batch_generator 2208441 2208441 2208400 2208400 2208400 100\n",
      "Train set: avg. loss: 0.638  (perplexity: 1.89)\n",
      "[epoch 10] in batch_generator 1100147 1100147 1100100 1100100 1100100 100\n",
      "Test set: avg. loss: 0.727  (perplexity: 2.07)\n",
      "\n",
      "Sat, 17 Dec 2016 21:42:07\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, './tf_saved/tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [1 if k == 3 else k for k in utils.flatten(y)]\n",
    "\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: 15687 out of 32432  correct, and total dev is  32513\n",
      "Accuracy rate is 0.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    pred = []\n",
    "\n",
    "    for s in test_sents:\n",
    "        pred.append(seq_predict(sm, session, s, vocab, svocab))\n",
    "\n",
    "    non0 = 0\n",
    "    correct = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        if not test_sentis[i][0] == '0':\n",
    "            non0 = non0 + 1\n",
    "            if pred[i] == test_sentis[i][0]:\n",
    "                correct = correct + 1\n",
    "print \"Test result:\", correct, 'out of', non0, ' correct, and total dev is ', len(test_sents)\n",
    "print \"Accuracy rate is %.2f\\n\" % (correct * 1.0/ non0)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
