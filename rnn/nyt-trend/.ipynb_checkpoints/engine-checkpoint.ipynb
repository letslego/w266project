{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade nltk\n",
    "#!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=6, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 20:20:08\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2146182 tokens)\n",
      "Test set: 32513 sentences (1067771 tokens)\n",
      "dev set: 32513 sentences (1066453 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sat, 17 Dec 2016 20:21:18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 1438 3026    4 5056    2    5    0   10  127  334  488    6  143  291\n",
      "  127  107   42  163 4131 1118  101   66  572  158 3410    5    0  898   16\n",
      "  139    7    4  886 1886    7    4  428 6915    4 1928    3   30   18  832\n",
      "    2    3   10 1034 1127  305    3 4665   60   85  762    3 1972    8   20\n",
      "   27 5689    5   30    0   16  373   14  214  452    8  452   13  747 1382\n",
      "    6  668   17   83    3  491    5    0  545  866   18   77   12   10  549\n",
      "   26   10 4169  868  164   42 1191    4  664    3]\n",
      "[2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0   63  638    3 1766   80    5    0   93  440  238   16 3215   14 2105\n",
      "  230 5011 8385 1075  532 1109    6 1007  118   17    2   23  228    3  118\n",
      "   33   16 1624    6    2   14 3738    9 1173  387    2   29    2  142   85\n",
      "    2    8 1368    2   15   34   21   53   49  183  372   31  770   26 2318\n",
      "  319   15  521   16   74  678    6 2641    2   19  253    8  237 1192 2574\n",
      "  204   34   69   71 1670 1350 4639   15  468   36   55   37    0   13  818\n",
      "    3    4 2555    7    4 2699   65 5560  981   14]\n",
      "[2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0  346 2278   47  564    6  943   14  304  513   77 2777    3   45 6824\n",
      "  202  534   79    4  277   11  549    3  170 2125   19    4  292   18   77\n",
      "    5    0 1915  364   43    6  592  209   40 2958    6   60 1636   27   10\n",
      " 1184  283    6  464  786   19    4  703  601    3   80    5   30    0    4\n",
      " 4607 1603    7   38    2   29  494  191   41 2124    4  178 2101   17 2413\n",
      "  783    5    0  211  119  111   18   12    4  299   31   10 5289  749    6\n",
      " 1577    4 7940  118 9498 4821    3    8  816   12]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[[u'last', u'March', u',', u'accusing', u'I.B.M', u'.']\n",
      " [u'General', u'Electric', u'Co', u'is', u'replacing', u'its', u'familiar', u'corporate', u'slogan', u\"'We\", u'bring', u'good', u'things', u'to', u'life', u\"'\", u'with', u\"'Imagination\", u'at', u'work', u',', u\"'\", u'which', u'is', u'meant', u'to', u'laud', u'its', u'innovations', u'in', u'areas', u'far', u'afield', u'from', u'prosaic', u'products', u'like', u'refrigerators', u'and', u'night', u'lights', u';', u'new', u'$', u'100', u'million', u'advertising', u'campaign', u'was', u'developed', u'by', u'BBDO', u'Worldwide', u';', u'change', u'is', u'also', u'intended', u'to', u'reflect', u're-emphasis', u'on', u'research', u'and', u'big', u'long-term', u'ideas', u'under', u'new', u'chief', u'executive', u'Jeffrey', u'R', u'Immelt', u';', u'photos', u'(', u'M', u')']\n",
      " [u'For', u'Sony', u',', u'the', u'importance', u'of', u'the', u'PlayStation', u'3', u'extends', u'beyond', u'its', u'battle', u'against', u'Microsoft', u'.']\n",
      " [u'PARC', u'pioneered', u'a', u'wide', u'range', u'of', u'technologies', u',', u'including', u'the', u'laser', u'printer', u',', u'the', u'Ethernet', u'office', u'network', u'and', u'the', u\"''graphical\", u'user', u'interface', u',', u\"''\", u'the', u'combination', u'of', u'multi-window', u'computer', u'screens', u'and', u'point-and-click', u'navigation', u'on', u'which', u'Apple', u'Computer', u\"'s\", u'Macintosh', u'and', u'Microsoft', u\"'s\", u'Windows', u'operating', u'system', u'were', u'modeled', u'.']\n",
      " [u'The', u'company', u'also', u'acknowledged', u'yesterday', u'the', u'billing', u\"''discrepancies\", u\"''\", u'in', u'previous', u'deals', u'with', u'its', u'two', u'largest', u'customers', u',', u'Wal-Mart', u'and', u'Lowe', u\"'s\", u',', u'a', u'home', u'improvement', u'retailer', u'.']\n",
      " [u'For', u'his', u'part', u',', u'he', u'said', u'leaving', u'Citigroup', u'would', u'allow', u'him', u'to', u'focus', u'more', u'freely', u'on', u'finding', u'a', u'chief', u'executive', u'job', u'in', u'financial', u'services', u'.']\n",
      " [u'William', u'S.', u'Grueskin', u',', u'the', u'managing', u'editor', u'of', u'The', u'Wall', u'Street', u'Journal', u'Online', u',', u'will', u'be', u'promoted', u'to', u'deputy', u'managing', u'editor', u'of', u'the', u'newspaper', u',', u'with', u'a', u'broad', u'responsibility', u'over', u'news', u'coverage', u'in', u'both', u'the', u'print', u'Journal', u'and', u'on', u'the', u'Web', u'site', u'.']\n",
      " [u'In', u'the', u'Dominican', u'Republic', u',', u'Am\\xe9rica', u'M\\xf3vil', u'said', u'it', u'would', u'spend', u'$', u'2.06', u'billion', u'for', u'100', u'percent', u'of', u'Verizon', u'Dominicana', u',', u'which', u'provides', u'landline', u',', u'wireless', u'and', u'Internet', u'service', u'.']\n",
      " [u'The', u'Microsoft', u'Network', u'on-line', u'service', u'arrived', u'with', u'Windows', u'95', u',', u'amid', u'much', u'blather', u'about', u'its', u'supposedly', u'simple', u'interface', u'.']\n",
      " [u'Microsoft', u'Must', u'Turn', u'To', u'New', u'Set', u'Of', u'Hurdles']\n",
      " [u'The', u'practice', u'evidently', u'did', u'not', u'extend', u'to', u'the', u'other', u'firms', u'that', u'settled', u'the', u'allegations', u',', u'including', u'some', u'that', u'are', u'Morgan', u'Stanley', u\"'s\", u'most', u'prominent', u'competitors', u'.']\n",
      " [u'IBM', u'will', u'announce', u'availability', u'of', u'first', u'of', u'new', u'family', u'of', u'Power', u'PC', u'microprocessors', u'designed', u'to', u'monitor', u'what', u'task', u'the', u'chip', u'is', u'performing', u'and', u'use', u'that', u'information', u'to', u'minimize', u'power', u'consumption', u';', u'chip', u'also', u'uses', u'hardware', u'to', u'handle', u'encryption', u'and', u'voice', u'recognition', u'functions', u'that', u'were', u'previously', u'done', u'with', u'software', u'(', u'S', u')']\n",
      " [u'Last', u'week', u',', u'MediaLive', u'said', u'it', u'was', u'establishing', u'an', u'advisory', u'group', u'of', u'technology', u'companies', u'to', u'help', u'redefine', u'the', u'event', u',', u'including', u'Advanced', u'Micro', u'Devices', u',', u'Borland', u'Software', u',', u'Microsoft', u',', u'Oracle', u',', u'Samsung', u'Electronics', u',', u'Cisco', u',', u'Dell', u'and', u'Intel', u'.']\n",
      " [u'Last', u'year', u',', u'Cisco', u\"'s\", u'annual', u'report', u'states', u',', u'52', u'million', u'options', u'were', u'canceled', u'.']\n",
      " [u'He', u'then', u'wondered', u'what', u'would', u'happen', u'if', u'his', u'company', u'stopped', u'distributing', u'Microsoft', u\"'s\", u'Web', u'browser', u'to', u'its', u'16', u'million', u'subscribers', u'and', u'replaced', u'it', u'with', u'Netscape', u\"'s\", u'browser', u'.']\n",
      " [u'By', u'contrast', u',', u'Comcast', u'was', u'valued', u'at', u'$', u'77', u'billion', u',', u'or', u'more', u'than', u'four', u'times', u'revenue', u'.']\n",
      " [u'Microsoft', u'will', u'not', u'say', u'precisely', u'what', u'its', u'remedy', u'recommendations', u'will', u'be', u'.']\n",
      " [u'Even', u'so', u',', u'the', u'Hidden', u'Valley', u'Ranch', u'co-brand', u'is', u'probably', u'the', u'last', u'between', u'the', u'two', u'companies', u'because', u'Clorox', u\"'s\", u'other', u'food', u'products', u',', u'like', u'Salad', u'Crispins', u'croutons', u',', u'do', u\"n't\", u'lend', u'themselves', u'to', u'snacks', u'.']\n",
      " [u'She', u'recalled', u'in', u'an', u'interview', u'that', u'at', u'an', u'I.B.M', u'.']\n",
      " [u'In', u'the', u'high', u'stakes', u'struggle', u'between', u'Newmont', u'Mining', u'of', u'Denver', u'and', u'South', u'Africa', u\"'s\", u'AngloGold', u'for', u'control', u'of', u'Australia', u\"'s\", u'largest', u'gold', u'producer', u',', u'Normandy', u'Mining', u',', u'Barrick', u'Gold', u'of', u'Canada', u'has', u'so', u'far', u'been', u'a', u'silent', u'spectator', u'.']\n",
      " [u'Not', u'too', u'long', u'ago', u',', u'such', u'titans', u'would', u'never', u'have', u'dreamed', u'of', u'looking', u'anywhere', u'but', u'I.B.M', u'.']\n",
      " [u'Against', u'this', u'opposition', u',', u'Microsoft', u'has', u'found', u'itself', u'in', u'the', u'uncommon', u'position', u'of', u'campaigning', u'for', u'the', u'even-handed', u'competition', u'of', u\"''a\", u'level', u'playing', u'field', u'.', u\"''\"]\n",
      " [u'The', u'common', u'view', u'is', u'that', u'Java', u'will', u'either', u'live', u'or', u'die', u'depending', u'on', u'whether', u'it', u'is', u'able', u'to', u'subvert', u'Microsoft', u\"'s\", u'control', u'of', u'the', u'personal', u'computer', u'.']\n",
      " [u'Many', u'Wall', u'Street', u'strategists', u'are', u'warning', u'investors', u'not', u'to', u'expect', u'technology', u'sector', u'to', u'turn', u'around', u'anytime', u'soon', u';', u'note', u'that', u'profits', u'at', u'first-tier', u'technology', u'companies', u'like', u'Cisco', u'Systems', u'and', u'EMC', u'have', u'fallen', u'as', u'fast', u'as', u'companies', u\"'\", u'stocks', u'and', u'that', u'many', u'big', u'technology', u'stocks', u'are', u'now', u'even', u'more', u'expensive', u'based', u'on', u'price-to-earnings', u'ratio', u'than', u'they', u'were', u'last', u'year', u'(', u'Market', u'Place', u'column', u')', u'(', u'M', u')']\n",
      " [u'High', u'manufacturing', u'yields', u'are', u'the', u'holy', u'grail', u'of', u'the', u'chip-making', u'industry', u'but', u'Intel', u'has', u'been', u'unable', u'to', u'translate', u'its', u'traditional', u'prowess', u'to', u'the', u'new', u'technology', u'.']\n",
      " [u'But', u'Microsoft', u'is', u'negotiating', u'hard', u'to', u'get', u'AT', u'&', u'T', u'to', u'allow', u'Windows', u'CE', u'to', u'be', u'introduced', u'in', u'at', u'least', u'one', u'large', u'cable', u'market', u'as', u'an', u'early', u'proving', u'ground', u'for', u'Microsoft', u\"'s\", u'set-top', u'box', u'technology', u',', u'according', u'to', u'the', u'people', u'close', u'to', u'the', u'talks', u'.']\n",
      " [u'Apple', u'has', u'not', u'sued', u'the', u'Web', u'sites', u'for', u'damages', u'for', u'publishing', u'the', u'trade', u'secrets', u',', u'but', u'it', u'could', u'try', u',', u'said', u'Eugene', u'Volokh', u',', u'a', u'law', u'professor', u'at', u'U.C.L.A', u'.']\n",
      " [u'Mr.', u'Klitten', u'also', u'said', u'that', u'Chevron', u'was', u'looking', u'to', u'become', u'a', u'major', u'refiner', u'and', u'marketer', u'of', u'gasoline', u'in', u'Europe', u'.']\n",
      " [u'The', u'suit', u'also', u'asserts', u'that', u'Marriott', u'secretly', u'and', u'deceptively', u'acquired', u'strategic', u',', u'confidential', u'information', u'on', u'the', u'rate', u'structures', u',', u'revenue', u',', u'profit', u'margins', u',', u'bookings', u'and', u'guest', u'lists', u'at', u'the', u'three', u'W.', u'H.', u'hotels', u'so', u'that', u'it', u'could', u'benefit', u'its', u'imminent', u'management', u'of', u'the', u'New', u'Orleans', u'Grand', u'Hotel', u',', u'formerly', u'known', u'as', u'Le', u'Meridien', u'New', u'Orleans', u'.']\n",
      " [u'EBay', u'purchased', u'Skype', u'in', u'2005', u'for', u'$', u'2.6', u'billion', u'.']]\n",
      "[['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']]\n",
      "2211208 2211208 1100285 1100285 1098967 1098967\n"
     ]
    }
   ],
   "source": [
    "print train_ids[:100]\n",
    "print train_sids[:100]\n",
    "print test_ids[:100]\n",
    "print test_sids[:100]\n",
    "print dev_ids[:100]\n",
    "print dev_sids[:100]\n",
    "print test_sents[:30]\n",
    "print test_sentis[:30]\n",
    "print len(train_ids), len(train_sids), len(test_ids), len(test_sids), len(dev_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = './tf_saved/tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 20:22:16\n",
      "WARNING:tensorflow:From <ipython-input-57-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:26:14\n",
      "[epoch 1] Completed in 0:03:56\n",
      "[epoch 1] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.711  (perplexity: 2.04)\n",
      "[epoch 1] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:31:19\n",
      "[epoch 2] Completed in 0:03:59\n",
      "[epoch 2] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.707  (perplexity: 2.03)\n",
      "[epoch 2] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:36:54\n",
      "[epoch 3] Completed in 0:04:26\n",
      "[epoch 3] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.703  (perplexity: 2.02)\n",
      "[epoch 3] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:42:24\n",
      "[epoch 4] Completed in 0:04:16\n",
      "[epoch 4] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.699  (perplexity: 2.01)\n",
      "[epoch 4] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.719  (perplexity: 2.05)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:47:48\n",
      "[epoch 5] Completed in 0:04:13\n",
      "[epoch 5] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.694  (perplexity: 2.00)\n",
      "[epoch 5] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.720  (perplexity: 2.05)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:53:11\n",
      "[epoch 6] Completed in 0:04:14\n",
      "[epoch 6] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.691  (perplexity: 1.99)\n",
      "[epoch 6] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.720  (perplexity: 2.06)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 20:58:34\n",
      "[epoch 7] Completed in 0:04:13\n",
      "[epoch 7] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.684  (perplexity: 1.98)\n",
      "[epoch 7] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.724  (perplexity: 2.06)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 21:03:54\n",
      "[epoch 8] Completed in 0:04:12\n",
      "[epoch 8] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.672  (perplexity: 1.96)\n",
      "[epoch 8] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.727  (perplexity: 2.07)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 21:09:18\n",
      "[epoch 9] Completed in 0:04:13\n",
      "[epoch 9] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.663  (perplexity: 1.94)\n",
      "[epoch 9] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.726  (perplexity: 2.07)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "in batch_generator 2211208 2211208 2211200 2211200 2211200 50\n",
      "Sat, 17 Dec 2016 21:14:56\n",
      "[epoch 10] Completed in 0:04:27\n",
      "[epoch 10] in batch_generator 2211208 2211208 2211200 2211200 2211200 100\n",
      "Train set: avg. loss: 0.653  (perplexity: 1.92)\n",
      "[epoch 10] in batch_generator 1098967 1098967 1098900 1098900 1098900 100\n",
      "Test set: avg. loss: 0.738  (perplexity: 2.09)\n",
      "\n",
      "Sat, 17 Dec 2016 21:16:15\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, './tf_saved/tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [1 if k == 3 else k for k in utils.flatten(y)]\n",
    "\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test result: 15780 out of 32431  correct, and total dev is  32513\n",
      "Accuracy rate is 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    pred = []\n",
    "\n",
    "    for s in test_sents:\n",
    "        pred.append(seq_predict(sm, session, s, vocab, svocab))\n",
    "\n",
    "    non0 = 0\n",
    "    correct = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        if not test_sentis[i][0] == '0':\n",
    "            non0 = non0 + 1\n",
    "            if pred[i] == test_sentis[i][0]:\n",
    "                correct = correct + 1\n",
    "print \"Test result:\", correct, 'out of', non0, ' correct, and total dev is ', len(test_sents)\n",
    "print \"Accuracy rate is %.2f\\n\" % (correct * 1.0/ non0)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
